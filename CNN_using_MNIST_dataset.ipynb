{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CNN using MNIST dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sampritich/samplerepo/blob/master/CNN_using_MNIST_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjfO9Y4ZSFWk",
        "colab_type": "text"
      },
      "source": [
        "# What is MNIST data?\n",
        "\n",
        "MNIST is a dataset consisting of 60,000+ images of handwritten digits for training and another 10,000 for testing. Each training example comes with an associated label (0 to 9) indicating what digit it is. Each digit will be a black and white image of 28 X 28 pixels.\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwGFBru6SFWo",
        "colab_type": "text"
      },
      "source": [
        "# Architecture of CNN\n",
        "![image.png](attachment:image.png)\n",
        "A convolutional neural network is different from a standard artificial neural network, and may involve convolutional, pooling, fully connected and softmax layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXFvAQ89SFWs",
        "colab_type": "text"
      },
      "source": [
        "# Tensorflow implementation\n",
        "\n",
        "# STEP 1: Importing Tensorflow:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkMBC6SWSFWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        },
        "outputId": "2ba58937-69b8-4989-da6c-00363a6c9659"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCtfLe6BSFW3",
        "colab_type": "text"
      },
      "source": [
        "# STEP 2: Importing the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR6qz_IUSFW6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "ed96b9b3-b136-43ce-f825-f84dc6d79ec4"
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-614640d4fa8f>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSNDWRywSFXE",
        "colab_type": "text"
      },
      "source": [
        "# STEP 3: Initializing the parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OkBUAW8SFXI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 10\n",
        "batch_size = 128\n",
        "\n",
        "x = tf.placeholder('float', [None, 784])\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "keep_rate = 0.8\n",
        "keep_prob = tf.placeholder(tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hru414_BSFXQ",
        "colab_type": "text"
      },
      "source": [
        "# STEP 4: Initializing the weights and bias parameters\n",
        "We'll define a weight and bias dictionary in this step. The dimensions for each layer need to be specified, in order to maintain consistency in the model. We use tf.random.normal in order to randomize the values initially.\n",
        "For the convolutional layers, we specify 5x5 filters. Next, we define the number of input and output features. For example, for W_conv2 ,the weights will have values [5,5,1,32]. We do similar definition for the biases as well. The fully connected layer will have 1024 parameters to train."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjPpxEUQSFXU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
        "               'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
        "               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
        "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
        "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
        "               'out':tf.Variable(tf.random_normal([n_classes]))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Zg90mRSFXa",
        "colab_type": "text"
      },
      "source": [
        "# STEP 5: Reshaping the input feature vector:\n",
        "The input feature vector, x, will need to be reshaped in order to fit the standard tensorflow syntax. Tensorflow takes 4D data as input for models, hence we need to specify it in 4D format. Each training example will be of 28X28 pixels. Hence, the tensorflow reshape function needs to be specified as:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "necbSnDlSFXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.reshape(x, shape=[-1, 28, 28, 1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPgjBBkKSFXk",
        "colab_type": "text"
      },
      "source": [
        "# STEP 6: Convolutional layer\n",
        "Convolutional layer is generally the first layer of a CNN. It calculates the element wise product of the image matrix, and a filter. In the above example, the image is a 5 x 5 matrix and the filter going over it is a 3 x 3 matrix. A convolution operation takes place between the image and the filter and the convolved feature is generated. The convoluted feature shown in the gif is the output of a convolutional layer.\n",
        "The function defined below will take the feature vector and weight vector as input and add a convolutional layer to our tensorflow model accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiabtOFdSFXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E07LZXWSFXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1']) #First conv layer\n",
        "\n",
        " conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2']) #Second conv layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syABi3DnSFX-",
        "colab_type": "text"
      },
      "source": [
        "# STEP 7: Pooling layer\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "Pooling layers are generally added after a convolutional layer, to reduce the dimensions of our data. A pooling window size is selected, and all the values in there will be replaced by the maximum, average or some other value. Max Pooling, one of the most common pooling techniques, may be demonstrated as follows:\n",
        "The following function is a maxpool function, taking the feature vector as input and adding a max-pooling layer to our model. The window size and strides are defined as shown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKzkkRilSFYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maxpool2d(x):\n",
        "    #                        size of window         movement of window\n",
        "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaRaQREvSFYI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv1 = maxpool2d(conv1)\n",
        "\n",
        "conv2 = maxpool2d(conv2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iM0YICQSFYO",
        "colab_type": "text"
      },
      "source": [
        "# STEP 8: Fully connected layer\n",
        "Now that our convolutional and pooling layers have reduced complexity of the data, we can use a regular fully connected layer in order to determine the true relation that our parameters have on labels. In order to classify the images as one label from 0 to 9, such a layer is needed. The data is flattenned to make it linear. These layers generally use the RELU activation function.\n",
        "\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTM5w4EpSFYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fc = tf.reshape(conv2,[-1, 7*7*64])\n",
        "fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "fc = tf.nn.dropout(fc, keep_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqDFOI5lSFYU",
        "colab_type": "text"
      },
      "source": [
        "# STEP 9: Softmax output layer\n",
        "\n",
        "Softmax is an output layer function which is used for multi-class classification problems. As we need an output layer which can classify a digit as either from 0 to 9, softmax function can be used to classify it to either of the 10 classes.\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_CbpI44SFaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "84997246-ca4c-4bdf-9645-29365e28faae"
      },
      "source": [
        "# Here we make use of a Softmax function and compare the result with the y label.\n",
        "cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(prediction,y) )\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-cdf5d02f31b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSGh2a7_SFaV",
        "colab_type": "text"
      },
      "source": [
        "# STEP 10: Optimizing and Training:\n",
        "An optimizer is used to refine the weights and biases, i.e. the parameters of the model so that the loss function is minimized. We'll use the 'Adam' Optimizer for our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJeerlljSFaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ptimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
        "                epoch_loss += c\n",
        "\n",
        "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIOoKlLaSFab",
        "colab_type": "text"
      },
      "source": [
        "# STEP 11: Accuracy of the model:\n",
        "In this step, we find the accuracy of the model in order to determine how well our model does on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8iWWmkzSFae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHXLqZfxSFaj",
        "colab_type": "text"
      },
      "source": [
        "# Complete Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVBPoW6VSFal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "b1d4de3b-abfa-4401-ec0a-244aa06d28c8"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)\n",
        "\n",
        "n_classes = 10\n",
        "batch_size = 128\n",
        "\n",
        "x = tf.placeholder('float', [None, 784])\n",
        "y = tf.placeholder('float')\n",
        "\n",
        "keep_rate = 0.8\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "def maxpool2d(x):\n",
        "    #                        size of window         movement of window\n",
        "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
        "\n",
        "\n",
        "def convolutional_neural_network(x):\n",
        "    weights = {'W_conv1':tf.Variable(tf.random_normal([5,5,1,32])),\n",
        "               'W_conv2':tf.Variable(tf.random_normal([5,5,32,64])),\n",
        "               'W_fc':tf.Variable(tf.random_normal([7*7*64,1024])),\n",
        "               'out':tf.Variable(tf.random_normal([1024, n_classes]))}\n",
        "\n",
        "    biases = {'b_conv1':tf.Variable(tf.random_normal([32])),\n",
        "               'b_conv2':tf.Variable(tf.random_normal([64])),\n",
        "               'b_fc':tf.Variable(tf.random_normal([1024])),\n",
        "               'out':tf.Variable(tf.random_normal([n_classes]))}\n",
        "\n",
        "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
        "\n",
        "    conv1 = tf.nn.relu(conv2d(x, weights['W_conv1']) + biases['b_conv1'])\n",
        "    conv1 = maxpool2d(conv1)\n",
        "    \n",
        "    conv2 = tf.nn.relu(conv2d(conv1, weights['W_conv2']) + biases['b_conv2'])\n",
        "    conv2 = maxpool2d(conv2)\n",
        "\n",
        "    fc = tf.reshape(conv2,[-1, 7*7*64])\n",
        "    fc = tf.nn.relu(tf.matmul(fc, weights['W_fc'])+biases['b_fc'])\n",
        "    fc = tf.nn.dropout(fc, keep_rate)\n",
        "\n",
        "    output = tf.matmul(fc, weights['out'])+biases['out']\n",
        "\n",
        "    return output\n",
        "\n",
        "def train_neural_network(x):\n",
        "    prediction = convolutional_neural_network(x)\n",
        "    p=tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y)\n",
        "    cost = tf.reduce_mean(p)\n",
        "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
        "    \n",
        "    hm_epochs = 10\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.initialize_all_variables())\n",
        "\n",
        "        for epoch in range(hm_epochs):\n",
        "            epoch_loss = 0\n",
        "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "                _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
        "                epoch_loss += c\n",
        "\n",
        "            print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
        "\n",
        "        correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
        "\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
        "        print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n",
        "\n",
        "train_neural_network(x)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-16-d26245b0ba71>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/tf_should_use.py:198: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
            "Instructions for updating:\n",
            "Use `tf.global_variables_initializer` instead.\n",
            "Epoch 0 completed out of 10 loss: 2040950.905380249\n",
            "Epoch 1 completed out of 10 loss: 296902.2223510742\n",
            "Epoch 2 completed out of 10 loss: 157616.07251739502\n",
            "Epoch 3 completed out of 10 loss: 103366.98542022705\n",
            "Epoch 4 completed out of 10 loss: 68128.96273803711\n",
            "Epoch 5 completed out of 10 loss: 49878.45234680176\n",
            "Epoch 6 completed out of 10 loss: 37820.993741989136\n",
            "Epoch 7 completed out of 10 loss: 26922.5341963768\n",
            "Epoch 8 completed out of 10 loss: 21909.017084122635\n",
            "Epoch 9 completed out of 10 loss: 19073.456674951594\n",
            "Accuracy: 0.9738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZs2q0cwSFaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}